{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "In this tutorial, we will learn serveral commonly used loss functons for regression, classification and other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'                # CPU only\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()              # Eager mode for TensorFlow\n",
    "\n",
    "tfe = tf.contrib.eager                   # it may become `tf.eager` or just use `tf` in 2.0+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For regression\n",
    "\n",
    "### L1 loss (absolute difference)\n",
    "\n",
    "$$\\ell_{sum}(\\hat{y}, y) = \\sum_{i=1}^{B}\\left|\\hat{y}_i - y_i\\right| $$\n",
    "$$\\ell_{mean}(\\hat{y}, y) = \\frac{1}{B}\\ell_{sum}(\\hat{y}, y)$$\n",
    "\n",
    "Here, $B$ is the batch size, $\\hat{y}$ is the outputs and $y$ is the real values.\n",
    "\n",
    "#### About the mean version \n",
    "\n",
    "The loss averaging over the batch so-called mean reduction is for two purposes:\n",
    "1. When adding regularization to the loss, mean reduction can scale them to a fixed ratio.\n",
    "2. If the batch size changes during the training, mean reducation may keep the optimization more stable.\n",
    "\n",
    "Moreover, when we update a weight $w$ with SGD,\n",
    "$$ w := w + \\lambda_{mean} \\frac{\\partial \\ell_{mean}}{\\partial w} = w + \\frac{\\lambda_{sum}}{B} \\frac{\\partial \\ell_{sum}}{\\partial w}.$$\n",
    "\n",
    "We will just use $\\ell = \\ell_{mean}$ in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6000, grad_fn=<L1LossBackward>)\n",
      "tensor([ 0.3333,  0.3333, -0.3333])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch\n",
    "\n",
    "criterion = nn.L1Loss(reduction='elementwise_mean')       # default; it will be 'mean' in 1.0+\n",
    "input = torch.tensor([1.6, 0.5, -2.], requires_grad=True)\n",
    "target = torch.tensor([0.8, 0, -1.5])\n",
    "loss = criterion(input, target)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8000, grad_fn=<L1LossBackward>)\n",
      "tensor([ 1.,  1., -1.])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.L1Loss(reduction='sum') \n",
    "input = torch.tensor([1.6, 0.5, -2.], requires_grad=True)\n",
    "target = torch.tensor([0.8, 0, -1.5])\n",
    "loss = criterion(input, target)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.59999996, shape=(), dtype=float32)\n",
      "tf.Tensor([ 0.33333334  0.33333334 -0.33333334], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow\n",
    "\n",
    "input = tfe.Variable([1.6, 0.5, -2.])\n",
    "target = [0.8, 0, -1.5]\n",
    "with tfe.GradientTape() as tape:\n",
    "    loss = tf.losses.absolute_difference(target, input, \n",
    "                                         reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS) # default\n",
    "grad = tape.gradient(loss, input)\n",
    "print(loss)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.8, shape=(), dtype=float32)\n",
      "tf.Tensor([ 1.  1. -1.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "input = tfe.Variable([1.6, 0.5, -2.])\n",
    "target = [0.8, 0, -1.5]\n",
    "with tfe.GradientTape() as tape:\n",
    "    loss = tf.losses.absolute_difference(target, input, \n",
    "                                         reduction=tf.losses.Reduction.SUM) \n",
    "grad = tape.gradient(loss, input)\n",
    "print(loss)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean squared error (squared L2 norm)\n",
    "\n",
    "$$\\ell = \\frac{1}{B} \\sum_i \\left(\\hat{y}_i - y_i\\right)^2$$\n",
    "\n",
    "It is more sensitive than L1 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3800, grad_fn=<MseLossBackward>)\n",
      "tensor([ 0.5333,  0.3333, -0.3333])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch\n",
    "criterion = nn.MSELoss()\n",
    "input = torch.tensor([1.6, 0.5, -2.], requires_grad=True)\n",
    "target = torch.tensor([0.8, 0, -1.5])\n",
    "loss = criterion(input, target)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.38000003, shape=(), dtype=float32)\n",
      "tf.Tensor([ 0.53333336  0.33333334 -0.33333334], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow\n",
    "\n",
    "input = tfe.Variable([1.6, 0.5, -2.])\n",
    "target = [0.8, 0, -1.5]\n",
    "with tfe.GradientTape() as tape:\n",
    "    loss = tf.losses.mean_squared_error(target, input) \n",
    "grad = tape.gradient(loss, input)\n",
    "print(loss)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber loss (Smooth L1 loss)\n",
    "\n",
    "$$\\ell = \\frac{1}{B} \\sum \\begin{cases}  0.5\\left(\\hat{y}_i-y_i\\right)^2, & \\text{ if } \\left|\\hat{y}_i-y_i\\right|<1 \\\\ \n",
    "\\left|\\hat{y}_i-y_i\\right|-0.5, & \\text{ otherwise} \\end{cases} $$\n",
    "\n",
    "It is less sensitive (robust) to outliers than the MSELoss and in some cases prevents exploding gradients (e.g. see “Fast R-CNN” paper by Ross Girshick).\n",
    "\n",
    "Pytorch:\n",
    "```python\n",
    "nn.SmoothL1Loss()\n",
    "```\n",
    "\n",
    "Tensorflow:\n",
    "```python\n",
    "tf.losses.huber_loss()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For classification\n",
    "\n",
    "### Negative log likelihood\n",
    "\n",
    "$$ \\ell = \\sum_{i=1}^{B} \\frac{-w_{y_i}}{\\sum_{i=1}^{B} w_{y_i}}\\hat{y}_{i,y_i} $$\n",
    "\n",
    "Pytorch:\n",
    "```python\n",
    "nn.NLLLoss()\n",
    "```\n",
    "Here, $\\hat{y}$ should be log-probabilities of each class. \n",
    "In Pytorch, we can use `nn.LogSigmoid` for binary classification or `nn.LogSoftmax` for multiclass classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -0.8096,  -1.3096,  -1.8096,  -2.1096],\n",
      "        [ -0.0002, -10.3002,  -9.6002,  -9.3002],\n",
      "        [ -4.5055,  -0.7055,  -6.7055,  -0.7055]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(0.6718, grad_fn=<NllLossBackward>)\n",
      "tensor([[ 0.1483, -0.2434,  0.0546,  0.0404],\n",
      "        [-0.0001,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0037,  0.1646,  0.0004, -0.1687]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "criterion = nn.NLLLoss()\n",
    "input = torch.tensor([[1.5, 1., 0.5, 0.2], \n",
    "                      [10., -0.3, 0.4, 0.7], \n",
    "                      [1.2, 5, -1., 5]], requires_grad=True)\n",
    "output = m(input)\n",
    "print(output)\n",
    "\n",
    "target = torch.tensor([1, 0, 3])\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4450, 0.2699, 0.1637, 0.1213],\n",
      "        [0.9998, 0.0000, 0.0001, 0.0001],\n",
      "        [0.0217, 0.0065, 0.0024, 0.9694]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(0.0645, grad_fn=<MseLossBackward>)\n",
      "tensor([[ 2.9858e-02, -3.4758e-02,  3.3075e-03,  1.5924e-03],\n",
      "        [-8.5331e-09,  1.2682e-09,  2.9385e-09,  4.3275e-09],\n",
      "        [ 1.8379e-04,  3.8858e-05,  1.2642e-05, -2.3529e-04]])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "criterion = nn.MSELoss()\n",
    "input = torch.tensor([[1.5, 1., 0.5, 0.2], \n",
    "                      [10.0, -0.3, 0.4, 0.7], \n",
    "                      [1.2, 0, -1., 5]], requires_grad=True)\n",
    "target = torch.tensor([[0, 1, 0, 0],\n",
    "                       [1, 0, 0, 0],\n",
    "                       [0, 0, 0, 1]])\n",
    "output = m(input)\n",
    "print(output)\n",
    "loss = criterion(output, target.float())\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss\n",
    "\n",
    "$$\\ell = \\sum_{i}\\frac{-w_{y_i}}{\\sum_{i}w_{y_i}} \\left(\\hat{y}_{i,y_i} - \\log\\left(\\sum_j \\exp\\left(\\hat{y}_{i, j}\\right) \\right) \\right)$$\n",
    "\n",
    "In Pytorch, `nn.CrossEntorpyLoss()` combines `nn.LogSoftmax()` and `nn.NLLLoss()`.\n",
    "\n",
    "In TensorFlow, `tf.losses.softmax_cross_entropy` can be used.\n",
    "\n",
    "For more detail about this loss function, please refer to http://neuralnetworksanddeeplearning.com/chap3.html#what_does_the_cross-entropy_mean_where_does_it_come_from\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "input = torch.tensor([[1.5, 1., 0.5, 0.2], \n",
    "                      [10., -0.3, 0.4, 0.7], \n",
    "                      [1.2, 5, -1., 5]], requires_grad=True)\n",
    "target = torch.tensor([1, 0, 3])\n",
    "loss = criterion(input, target)\n",
    "loss.backward()\n",
    "print(loss)\n",
    "print(input.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Binary Cross entropy loss\n",
    "\n",
    "$$\\ell = \\frac{1}{B}\\sum_{i}-w_{i} \\left[y_i\\log\\hat{y}_{i} - \\left(1 - y_i\\right) \\log\\left( 1 - \\hat{y}_i \\right) \\right]$$\n",
    "\n",
    "Pytorch:\n",
    "```python\n",
    "nn.BCELoss()\n",
    "```\n",
    "\n",
    "TensorFlow:\n",
    "```python\n",
    "tf.losses.sigmoid_cross_entropy()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For ranking\n",
    "\n",
    "### Margin ranking loss (Hinge loss)\n",
    "\n",
    "$$\\ell = \\frac{1}{B}\\sum_{i} \\max\\left(0, -y \\left(\\hat{y}_{i, 1} - \\hat{y}_{i, 2}\\right)\\right) + m $$\n",
    "\n",
    "Here,  if $y=1$ then it assumed that $\\hat{y}_1$ shoud be ranked heigher than $\\hat{y}_2$, and vice-versa for $y = -1$; $m$ is the margin between the two inputs.\n",
    "\n",
    "Pytorch:\n",
    "```python\n",
    "nn.MarginRankingLoss()\n",
    "```\n",
    "Also see `nn.TripletMarginLoss()`(anchor-based) and `nn.MultiMarginLoss()`(for multi-class classification).\n",
    "\n",
    "Tensorflow:\n",
    "```python\n",
    "tf.losses.hinge_loss()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For similarity\n",
    "\n",
    "### Cosine loss\n",
    "$$\\ell = \\frac{1}{B}\\sum_i \\begin{cases} 1 - \\cos\\left(\\hat{y}_{i,1},\\hat{y}_{i,2} \\right),& \\text{ if similar} \\\\ \\max\\left(0, \\cos\\left(\\hat{y}_{i,1},\\hat{y}_{i,2} \\right) - m\\right),& \\text{if dissimilar} \\end{cases} $$\n",
    "\n",
    "Pytorch:\n",
    "```python\n",
    "nn.CosineEmbeddingLoss()\n",
    "```\n",
    "\n",
    "TensorFlow:\n",
    "```python\n",
    "tf.losses.cosine_distance()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For distribution\n",
    "\n",
    "### Kullback-Leibler (KL) divergence\n",
    "\n",
    "$$\\begin{align}\n",
    "\\ell &= \\frac{1}{B}\\sum_i y_i\\log\\frac{y_i}{\\hat{y}_i} \\\\\n",
    "     &= \\frac{1}{B}\\sum_i \\left(y_i\\log y_i - y_i\\log\\hat{y}_i\\right)\\end{align}$$\n",
    "     \n",
    "Pytroch:\n",
    "```python\n",
    "nn.KLDivLoss()             # the input given is expected to contain log-probabilities\n",
    "```\n",
    "\n",
    "TensorFlow:\n",
    "```python\n",
    "tf.distributions.kl_divergence()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For count\n",
    "\n",
    "### Log Poisson loss\n",
    "\n",
    "$$\\begin{align}\n",
    "\\ell &= \\frac{1}{B}\\sum_i \\left[\\hat{y}_i - y\\log\\hat{y} + \\log \\left(y!\\right) \\right]\\\\\n",
    "     &\\approx \\frac{1}{B}\\sum_i \\left[\\hat{y} - y\\log\\hat{y} + 0.5\\log(2\\pi y)\\right]\n",
    "\\end{align}$$\n",
    "\n",
    "It assumes that $y \\sim P(\\hat{y})$. The $0.5\\log(2\\pi y)$ is [Stirling's approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation). It can be used for $y > 1$ (compute full loss).\n",
    "\n",
    "\n",
    "Pytorch:\n",
    "```python\n",
    "nn.PoissonNLLLoss()                 # full=False in default\n",
    "```\n",
    "\n",
    "TensorFlow:\n",
    "```python\n",
    "tf.nn.log_poisson_loss()            # compute_full_loss=False in default\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLLecture",
   "language": "python",
   "name": "dllecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
