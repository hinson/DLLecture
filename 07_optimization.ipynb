{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "In this tutorial, we will see how gradient descent optimizes a model with a simple example in Pytorch.\n",
    "\n",
    "More examples in TensorFlow: https://github.com/Jaewan-Yun/optimizer-visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"       \n",
    "\n",
    "DEVICE = 'cpu' #'cuda:0'\n",
    "\n",
    "REAL_PARAMS = [1.2, 2.5]\n",
    "\n",
    "FIG_NUM = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "The model we need to fit is $$ y=\\sin \\left( b \\cos\\left(ax\\right) \\right). $$\n",
    "Here, $a$ and $b$ are the parameters to optimize.\n",
    "\n",
    "The criterion for computing cost is mean squared error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "X = torch.linspace(-1, 1, num_samples, device=device)\n",
    "f = lambda x, a, b: torch.sin(b * torch.cos(a * x))\n",
    "noise = torch.randn(num_samples, device=device) / 10\n",
    "y = f(X, *REAL_PARAMS) + noise\n",
    "\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the 3D loss landscape for all samples as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "FIG_NUM += 1\n",
    "fig = plt.figure(FIG_NUM, figsize=(8, 6))\n",
    "ax = fig.gca(projection='3d')\n",
    "a3D, b3D = np.meshgrid(np.linspace(-3, 10, 100), np.linspace(-3, 10, 100))  # parameter space\n",
    "cost3D = np.array([criterion(f(X, float(a_), float(b_)), y).detach().cpu().numpy()\n",
    "                   for a_, b_ in zip(a3D.flatten(), b3D.flatten())]).reshape(a3D.shape)\n",
    "ax.plot_surface(a3D, b3D, cost3D, rstride=1, cstride=1, cmap='jet', alpha=0.7)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from statistics import mean\n",
    "\n",
    "# define the levels to show in contour ploting\n",
    "CONTOUR_LEVELS = np.concatenate((np.linspace(0, 0.1, 5, endpoint=False),\n",
    "                                 np.linspace(0.1, 1, 4, endpoint=False),\n",
    "                                 np.linspace(1, 2, 5, endpoint=False),\n",
    "                                 np.linspace(2, 4, 5)))\n",
    "\n",
    "\n",
    "def train(init_params, epochs, lr, batch_size, weight_decay, sgdm_momentum, lr_lambda):\n",
    "    \n",
    "    # training data\n",
    "    dataloader = torch.utils.data.DataLoader(list(zip(X, y)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # optimizers\n",
    "    get_optimizer_dict = {\n",
    "        'sgd': lambda p: optim.SGD(p, lr=lr, weight_decay=weight_decay),\n",
    "        'sgdm': lambda p: optim.SGD(p, lr=lr, momentum=sgdm_momentum, weight_decay=weight_decay),\n",
    "        'adgrad': lambda p: optim.Adagrad(p, lr=lr, weight_decay=weight_decay),\n",
    "        'rmsprop': lambda p: optim.RMSprop(p, lr=lr, weight_decay=weight_decay),\n",
    "        'adam': lambda p: optim.Adam(p, lr=lr, weight_decay=weight_decay),\n",
    "        'adamax': lambda p: optim.Adamax(p, lr=lr, weight_decay=weight_decay),\n",
    "    }\n",
    "    \n",
    "    # recorders of parameters and cost\n",
    "    a_list, b_list, cost_list = defaultdict(list), defaultdict(list), defaultdict(list) \n",
    "\n",
    "    for name, get_optimizer in get_optimizer_dict.items():\n",
    "        \n",
    "        # initialize parameters\n",
    "        a, b = [torch.tensor(float(p), device=device, requires_grad=True) for p in init_params]\n",
    "        \n",
    "        # get an optimizer\n",
    "        optimizer = get_optimizer((a, b))\n",
    "        \n",
    "        # scheduler of changing the learning rate by epoch\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            scheduler.step()   # change the learning rate\n",
    "                        \n",
    "            for input, target in dataloader:\n",
    "                # record parameters\n",
    "                a_list[name].append(float(a))\n",
    "                b_list[name].append(float(b))    \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output = f(input, a, b)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                # record cost\n",
    "                cost_list[name].append(float(loss))\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    return a_list, b_list, cost_list\n",
    "\n",
    "def plot_fit_functions(a_list, b_list):\n",
    "    global FIG_NUM\n",
    "\n",
    "    FIG_NUM +=1\n",
    "    plt.figure(FIG_NUM, figsize=(5, 4))\n",
    "    \n",
    "    plt.scatter(X.cpu().numpy(), y.cpu().numpy(), s=5)    # training data\n",
    "\n",
    "    lines = []\n",
    "    for name in a_list.keys():\n",
    "        pred = f(X, a_list[name][-1], b_list[name][-1]).detach().cpu().numpy()       # prediction\n",
    "        line, = plt.plot(X.cpu().numpy(), pred, '-', lw=1, alpha=.7)                 # fit curve          \n",
    "        lines.append(line)\n",
    "\n",
    "    plt.legend(lines, a_list.keys())\n",
    "\n",
    "def plot_optimization_steps_2d(a_list, b_list, cost_list):\n",
    "    global FIG_NUM\n",
    "\n",
    "    FIG_NUM +=1\n",
    "    plt.figure(FIG_NUM, figsize=(8, 6))\n",
    "    \n",
    "    CS = plt.contour(a3D, b3D, cost3D, CONTOUR_LEVELS, cmap='jet')\n",
    "    plt.clabel(CS, inline=1)\n",
    "\n",
    "    plt.scatter(a_list['sgd'][0], b_list['sgd'][0],  s=30)    # initial parameter place\n",
    "    plt.xlabel('a'); \n",
    "    plt.ylabel('b')\n",
    "\n",
    "    lines = []\n",
    "    for name in a_list.keys(): \n",
    "        line, = plt.plot(a_list[name], b_list[name], lw=1, alpha=.7)    # plot 2d gradient descent\n",
    "        lines.append(line)\n",
    "\n",
    "    plt.legend(lines, a_list.keys())\n",
    "\n",
    "def plot_optimization_steps_3d(a_list, b_list, cost_list):\n",
    "    global FIG_NUM\n",
    "\n",
    "    FIG_NUM +=1\n",
    "    fig = plt.figure(FIG_NUM, figsize=(8, 6))\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    ax.scatter(a_list['sgd'][0], b_list['sgd'][0], zs=cost_list['sgd'][0], s=50, c='r')  # initial parameter place\n",
    "    ax.set_xlabel('a')\n",
    "    ax.set_ylabel('b')\n",
    "    \n",
    "    ax.plot_surface(a3D, b3D, cost3D, rstride=1, cstride=1, cmap='jet', alpha=0.7)\n",
    "\n",
    "\n",
    "    lines = []\n",
    "    for name in a_list.keys(): \n",
    "        line, = ax.plot(a_list[name], b_list[name], zs=cost_list[name], zdir='z', lw=1)    # plot 3D gradient descent\n",
    "        lines.append(line)\n",
    "\n",
    "    plt.legend(lines, a_list.keys())\n",
    "    plt.show()\n",
    "    \n",
    "def do_experiments(init_params=(2., 4.5), epochs=50, lr=0.01, batch_size=16, weight_decay=0, sgdm_momentum=0.9,\n",
    "                   lr_lambda=lambda epoch: 1):\n",
    "    lists = train(init_params, epochs, lr, batch_size, weight_decay, sgdm_momentum, lr_lambda)\n",
    "    plot_fit_functions(*lists[:2])\n",
    "    plot_optimization_steps_2d(*lists)\n",
    "    \n",
    "    return lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1\n",
    "\n",
    "Initial parameters: $a = 2$, $b = 4.5$\n",
    "\n",
    "Epochs: 50\n",
    "\n",
    "Learning rate: 0.01\n",
    "\n",
    "Batch size: 16\n",
    "\n",
    "Coefficient of L2 regularization (weight decay): 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = do_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_steps_3d(*lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2\n",
    "\n",
    "Initial parameters: $a = 2$, $b = 4.5$\n",
    "\n",
    "Epochs: 50\n",
    "\n",
    "Learning rate: 0.01\n",
    "\n",
    "Batch size: 16\n",
    "\n",
    "Coefficient of L2 regularization (weight decay): 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiments(weight_decay=0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "\n",
    "Initial parameters: $a = 2$, $b = 4.5$\n",
    "\n",
    "Epochs: 200\n",
    "\n",
    "Learning rate: $0.01 \\times 0.9^{epoch}$\n",
    "\n",
    "Batch size: 16\n",
    "\n",
    "Coefficient of L2 regularization (weight decay): 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiments(epochs=200, lr_lambda=lambda epoch: 0.9**epoch);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4\n",
    "\n",
    "Initial parameters: $a = 2$, $b = 4.5$\n",
    "\n",
    "Epochs: 200\n",
    "\n",
    "Learning rate: $0.02 \\times 0.9^{epoch}$\n",
    "\n",
    "Batch size: 16\n",
    "\n",
    "Coefficient of L2 regularization (weight decay): 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiments(lr=0.02, weight_decay=0.001, epochs=200, lr_lambda=lambda epoch: 0.9**epoch);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5\n",
    "\n",
    "Initial parameters: $a = 5$, $b = 3$\n",
    "\n",
    "Epochs: 200\n",
    "\n",
    "Learning rate: $0.1 \\times 0.9^{epoch}$\n",
    "\n",
    "Batch size: 16\n",
    "\n",
    "Coefficient of L2 regularization (weight decay): $1\\times10^{-4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_experiments((5., 3.), lr=0.1, epochs=200, weight_decay=0.0001,\n",
    "               lr_lambda=lambda epoch: 0.9**epoch);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for parameter initialization\n",
    "\n",
    "### Xavier: Tanh, Sigmoid\n",
    "Xavier samples values from $\\mathcal{N}(0, \\sigma)$.\n",
    "\n",
    "$$\\sigma = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{#in} + \\text{#out}}}$$\n",
    "\n",
    "### Kaiming: ReLU, Leaky ReLU\n",
    "\n",
    "Kaiming samples values from $\\mathcal{N}(0, \\sigma)$\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{#in}}}$$\n",
    "\n",
    "Here, $a$ is the negative slope for Leaky ReLU (0 for ReLU).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = nn.Linear(5, 5)\n",
    "print(module.weight)\n",
    "nn.init.kaiming_normal_(module.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "DLLecture",
   "language": "python",
   "name": "dllecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
