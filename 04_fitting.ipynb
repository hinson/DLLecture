{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting in Neural Network\n",
    "\n",
    "This tutorial is adapted from [visual proof of fitting](http://neuralnetworksanddeeplearning.com/chap4.html).\n",
    "\n",
    "In this tutorial, we can learn how neural network fits a continuous function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['seaborn-whitegrid'])\n",
    "\n",
    "import sys\n",
    "sys.path.append('scripts')\n",
    "from draw_neural_net import draw_neural_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "\n",
    "Let us use sigmoid function as the activation function in the following experiments.\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + \\exp{(-z)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(z, sigmoid(z), '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and biases in neural network\n",
    "\n",
    "Consider a neural network having one hidden layer with two hidden neurons and sigmoid activation function which only performs at the output layer, i.e., \n",
    "\n",
    "$$h_1^{(1)} = w_1^{(1)}x + b_1^{(1)}$$\n",
    "$$h_2^{(1)} = w_2^{(1)}x + b_2^{(1)} $$\n",
    "$$y = \\sigma\\left(w_1^{(2)}h_1^{(1)}  + w_2^{(2)}h_2^{(1)}+ b^{(2)}\\right )$$\n",
    "\n",
    ", where $h$ denotes hidden neuron, $w$ and $b$ denotes weight and bias, ($\\cdot$) is the number of layer, $y$ is the output and $\\sigma$ indicates the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_121 = plt.figure(figsize=(9, 7))\n",
    "ax = nn_121.gca()\n",
    "ax.axis('off')\n",
    "draw_neural_net(ax, .1, .8, .1, .8, [1, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_nn2d(input, weight, bias, activate=sigmoid, \n",
    "            unactivated=False, activated_output=True, ylim=(0, 1.1)):\n",
    "\n",
    "    x = h = input\n",
    "    num_layers = len(weight)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        if not unactivated and i != 0:\n",
    "            h = activate(h)\n",
    "        \n",
    "        w = weight[i]\n",
    "        b = bias[i]\n",
    "        h = h.dot(w) + b\n",
    "        \n",
    "    y = h\n",
    "    if activated_output:\n",
    "        y = activate(h)\n",
    "        \n",
    "        \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    ax_plt = plt.subplot(121)\n",
    "    ax_plt.set_title('')\n",
    "    plt.plot(x, y, '-')\n",
    "    plt.ylim(*ylim)\n",
    "    \n",
    "    ax_nn = plt.subplot(122)\n",
    "    ax_nn.axis('off')\n",
    "    \n",
    "    layer_sizes = []\n",
    "    layer_sizes.append(input.shape[1])\n",
    "    \n",
    "\n",
    "    for i in range(num_layers):\n",
    "        layer_sizes.append(weight[i].shape[1])\n",
    "    draw_neural_net(ax_nn, .1, .8, .1, .8, layer_sizes, weight, bias)\n",
    "    \n",
    "    return (y, ax_plt, ax_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Drag the sliders of $w^{(1)}_1$ and $b^{(1)}_1$, and see how the curve changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn121_simple(w=19, b=17):\n",
    "    x = np.linspace(0, 1, 50)[:, np.newaxis]\n",
    "    weight = [np.array([[w, 0]]), np.array([[1], [0]])]\n",
    "    bias = [[b, 0], [0]]\n",
    "    ax_plt = plot_nn2d(x, weight, bias, unactivated=True)[1]     \n",
    "    if w != 0:\n",
    "        ax_plt.set_title('$s=-b/w={:.2f}$'.format(-b/w))\n",
    "    \n",
    "\n",
    "interactive_plot = interactive(plot_nn121_simple, w=(-1000, 1000, 1), b=(-1000, 1000, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '550px'\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the value $s = -b/w$ controls the position of the step.\n",
    "\n",
    "Now, The activation functions in the following neural networks perform at _every_ hidden layer _expect for_ the output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn121_step(s11=.4, s12=.6, w21=.6, w22=1.2):\n",
    "    x = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "    weight = [1000*np.array([[1, 1]]), np.array([[w21], [w22]])]\n",
    "    bias = [1000*np.array([-s11, -s12]), [0]]\n",
    "    ax_plt = plot_nn2d(x, weight, bias, activated_output=False, ylim=(-1, 2))[1]\n",
    "\n",
    "interactive_plot = interactive(plot_nn121_step, \n",
    "                               s11=(-2., 2., .1), s12=(-2., 2., .1),\n",
    "                               w21=(-5., 5., .1), w22=(-5., 5., .1))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '550px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn121_bump(w2=.6):\n",
    "    x = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "    weight = [1000*np.array([[1, 1]]), np.array([[w2], [-w2]])]\n",
    "    bias = [[-400, -600], [0]]\n",
    "    ax_plt = plot_nn2d(x, weight, bias, activated_output=False, ylim=(-1, 2))[1]\n",
    "\n",
    "\n",
    "interactive_plot = interactive(plot_nn121_bump, w2=(-3., 3., .1))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '550px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn141_bump2(s1=.4, s2=.6, s3=.7, s4=.9, h1=-.8, h2=.9):\n",
    "    x = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "    weight = [1000*np.array([[1, 1, 1, 1]]), np.array([[h1], [-h1], [h2], [-h2]])]\n",
    "    bias = [1000*np.array([-s1, -s2, -s3, -s4]), [0]]\n",
    "    ax_plt = plot_nn2d(x, weight, bias, activated_output=False, ylim=(-1, 2))[1]\n",
    "\n",
    "interactive_plot = interactive(plot_nn141_bump2, \n",
    "                               s1=(-3., 3., .1), s2=(-3., 3., .1),\n",
    "                               s3=(-3., 3., .1), s4=(-3., 3., .1),\n",
    "                               h1=(-5., 5., .1), h2=(-5., 5., .1))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '550px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a continuous function\n",
    "\n",
    "Given a toy function\n",
    "$$f(x) = 0.2 + 0.4x^2 + 0.3x \\sin(15x) + 0.05\\cos(50x)$$, use a neural network to fit it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return .2 + .4 * x ** 2 + .3 * x * np.sin(15*x) + .05 * np.cos(50*x)\n",
    "\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, f(x), '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from a general $l$-layer neural network can be\n",
    "\n",
    "$$y = \\sigma\\left(\\sum_j{w_j^{(l)} a_j^{(l-1)}} + b^{(l)}\\right)$$\n",
    "\n",
    ", where $\\sigma$ is the activation function, $a$ denotes an activated hidden neuron.\n",
    "\n",
    "Fitting the function $f$ implies that $y$ need to be approximate to $f(x)$ as close as possible by given $x$, i.e, \n",
    "\n",
    "$$ y \\simeq f(x)$$\n",
    "\n",
    "$$\\sum_j{w_j^{(l)} a_j^{(l-1)}} + b^{(l)} \\simeq \\sigma^{-1}\\left(f(x)\\right) $$\n",
    "\n",
    "**Notice**:  $\\sigma^{-1}$ may not be everywhere defined over $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit function\n",
    "The inverse function of sigmoid is given by\n",
    "$$\\sigma^{-1}(y) = \\ln{(y)} - \\ln{(1-y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(y):\n",
    "    \"\"\"inverse function of sigmoid\"\"\"\n",
    "    return np.log(y) - np.log(1 - y)\n",
    "\n",
    "y = np.linspace(1e-3, .999, 100)\n",
    "plt.plot(y, logit(y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot $\\sigma^{-1}(f)$ (the _yellow_ one) as follows since the range of $f$ (the blue one) is in $(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, f(x))\n",
    "plt.plot(x, logit(f(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error function\n",
    "\n",
    "We use the following function to quantify the deviation between the output and the real values:\n",
    "\n",
    "$$L = \\frac{1}{n}\\sum_{i=1}^{n}\\left|y-y'\\right|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(y, y_):\n",
    "    return np.abs(y - y_).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Try to let the average deviation be less than 0.4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn1x1_bump5(h1=.4, h2=-.3, h3=-.2, h4=-.4, h5=-.5):\n",
    "    x = np.linspace(0, 1, 100)[:, np.newaxis]\n",
    "    weight = [1000*np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), \n",
    "              np.array([[h1], [-h1], [h2], [-h2], [h3], [-h3], [h4], [-h4], [h5], [-h5]])]\n",
    "    bias = [-1000*np.array([0, .2, .2, .4, .4, .6, .6, .8, .8, 1.]), [0]]\n",
    "    y, ax_plt = plot_nn2d(x, weight, bias, activated_output=False, ylim=(-3, 3))[:2]\n",
    "    y_ = logit(f(x))\n",
    "    ax_plt.plot(x, y_)\n",
    "    deviation = error(y, y_)\n",
    "    title_templ = '$error = {:.3f}$'\n",
    "    \n",
    "    if deviation < 0.4:\n",
    "        title_templ += ' Good Job!'\n",
    "        \n",
    "    ax_plt.set_title(title_templ.format(deviation))\n",
    "\n",
    "\n",
    "# Fill the arguments!\n",
    "plot_nn1x1_bump5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_21 = plt.figure(figsize=(10, 10))\n",
    "ax = nn_21.gca()\n",
    "ax.axis('off')\n",
    "draw_neural_net(ax, .1, .8, .1, .8, [2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_nn21(w1=8, b=-4):\n",
    "\n",
    "    x1 = np.linspace(0, 1, 100)\n",
    "    x2 = np.linspace(0, 1, 100)\n",
    "    x1, x2 = np.meshgrid(x1, x2)\n",
    "    y = sigmoid(x1.T*w1 + b)\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(x1, x2, y)\n",
    "    ax.set_title('$s_x=-b/w_1={:.2f}$'.format(-b/(w1+1e-6)))\n",
    "    return ax\n",
    "\n",
    "interactive_plot = interactive(plot_nn21, w1=(-1000., 1000., .1), b=(-1000., 1000., .1))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '550px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_241 = plt.figure(figsize=(16, 12))\n",
    "ax = nn_241.gca()\n",
    "ax.axis('off')\n",
    "\n",
    "weights = [np.array([[1000, 1000, 0, 0], \n",
    "                      [0, 0, 1000, 1000]]),\n",
    "           np.array([[.3, -.3, .3, -.3]]).T]\n",
    "\n",
    "biases = [[-400, -600, -300, -700], [0]]\n",
    "draw_neural_net(ax, .1, .8, .1, .8, [2, 4, 1], weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn241(w=1, b=0):\n",
    "\n",
    "    x1 = np.linspace(0, 1, 100)\n",
    "    x2 = np.linspace(0, 1, 100)\n",
    "    x1, x2 = np.meshgrid(x1, x2)\n",
    "    \n",
    "    y = sigmoid(x1.T * 1000 - 400) * w + sigmoid(x1.T * 1000 - 600) * -w + \\\n",
    "        sigmoid(x2.T * 1000 - 300) * w + sigmoid(x2.T * 1000 - 700) * -w + b\n",
    "    \n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(x1, x2, y)\n",
    "    ax.set_zlim(-3, 3)\n",
    "    ax.set_title('$w={}, b={}$'.format(w, b))\n",
    "    return ax\n",
    "\n",
    "interactive_plot = interactive(plot_nn241, w=(-10, 10, 1), b=(-10, 10, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '550px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_2821 = plt.figure(figsize=(16, 12))\n",
    "ax = nn_2821.gca()\n",
    "ax.axis('off')\n",
    "\n",
    "weights = [np.array([[1000, 1000, 0, 0, 1000, 1000, 0, 0], \n",
    "                      [0, 0, 1000, 1000, 0, 0, 1000, 1000]]),\n",
    "           np.array([[.8, 0], [-.8, 0], [.8, 0], [-.8, 0],\n",
    "                    [0, .5], [0, -.5], [0, .5,], [0, -.5]]),\n",
    "           np.array([[.7, .5]]).T]\n",
    "\n",
    "biases = [\n",
    "    [-600, -400, 400, -500, -800, -700, 300, -900],\n",
    "    [0, 0],\n",
    "    [0],\n",
    "]\n",
    "\n",
    "draw_neural_net(ax, .1, .8, .1, .8, [2, 8, 2, 1], weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nn2821(s11=.6, s12=.4, s23=-.4, s24=.5, s15=.8, s16=.7, s27=-.3, s28=.9, \n",
    "                w2_1=.8, w2_2=.5, w31=.7, w32=.5, b=0):\n",
    "\n",
    "    x1 = np.linspace(0, 1, 100)\n",
    "    x2 = np.linspace(0, 1, 100)\n",
    "    x1, x2 = np.meshgrid(x1, x2)\n",
    "    \n",
    "    y = (\n",
    "            sigmoid(x1.T * 1000 - 1000*s11) * w2_1 + sigmoid(x1.T * 1000 - 1000*s12) * -w2_1 + \\\n",
    "            sigmoid(x2.T * 1000 - 1000*s23) * w2_1 + sigmoid(x2.T * 1000 - 1000*s24) * -w2_1 + b\n",
    "        ) * w31 + \\\n",
    "        (\n",
    "            sigmoid(x1.T * 1000 - 1000*s15) * w2_2 + sigmoid(x1.T * 1000 - 1000*s16) * -w2_2 + \\\n",
    "            sigmoid(x2.T * 1000 - 1000*s27) * w2_2 + sigmoid(x2.T * 1000 - 1000*s28) * -w2_2 + b\n",
    "        ) * w32\n",
    "    \n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_surface(x1, x2, y)\n",
    "    ax.set_zlim(-3, 3)\n",
    "    return ax\n",
    "\n",
    "interactive_plot = interactive(plot_nn2821, \n",
    "                               s11=(-1., 1., .1), s12=(-1., 1., .1),\n",
    "                               s23=(-1., 1., .1), s24=(-1., 1., .1),\n",
    "                               s15=(-1., 1., .1), s16=(-1., 1., .1), \n",
    "                               s27=(-1., 1., .1), s28=(-1., 1., .1),\n",
    "                               w21=(-10, 10, 1),w22=(-10, 10, 1),\n",
    "                               w31=(0., 1., .1),w32=(0., 1., .1),\n",
    "                               b=(-10, 10, 1))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '550px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework**: Try to change the structure of neural network and even the activation function (Tanh, ReLu, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "DLLecture",
   "language": "python",
   "name": "dllecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
