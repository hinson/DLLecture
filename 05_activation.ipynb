{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "In this tutorial, we will compare several commonly used activation functions - Sigmoid, Tahn, ReLU and Leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # so the IDs match nvidia-smi\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"       # eg. \"0, 1, 2\" for multiple\n",
    "\n",
    "\n",
    "DATA_ROOT = '/data1/mnist/'\n",
    "DEVICE = 'cuda:0'\n",
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spells...\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.reload_library()\n",
    "plt.style.use(['seaborn-whitegrid'])\n",
    "plt.ion()    # interactive mode: on\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import tensorboardX\n",
    "\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the handwritten digits dataset (MNIST) in following experiments.\n",
    "\n",
    "Each image in MNIST is a $1\\times28\\times28$ tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(root=DATA_ROOT, train=True, #download=True#,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ])), batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# Test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST(root=DATA_ROOT, train=False,\n",
    "                                transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                            ])), batch_size=TEST_BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a MLP with a specified activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size, out_size, hidden_size=64, num_hidden_layers=5, activation='sigmoid', leaky_a=0.01):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # select activation module by name\n",
    "        activation_dict = {\n",
    "            'sigmoid': nn.Sigmoid(),\n",
    "            'tanh': nn.Tanh(),\n",
    "            'relu': nn.ReLU(),\n",
    "            'leaky_relu': nn.LeakyReLU(leaky_a),\n",
    "        }\n",
    "        self.activation = activation_dict[activation]\n",
    "        \n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        num_connections = num_hidden_layers - 1\n",
    "        \n",
    "        if isinstance(hidden_size, int):\n",
    "            hidden_sizes = [hidden_size] * num_connections\n",
    "        elif type(hidden_size) in (list, tuple) and len(hidden_size) == num_connections:\n",
    "            hidden_sizes = hidden_size\n",
    "        else:\n",
    "            raise Exception('hidden_size must be a integer or a list/tuple with size {}!'.format(num_connections))\n",
    "                  \n",
    "        layers = []\n",
    "        \n",
    "        # the first layer\n",
    "        layers.append(nn.Linear(in_size, hidden_sizes[0]))\n",
    "        layers.append(self.activation)\n",
    "        \n",
    "        # the middle layers\n",
    "        for i in range(num_connections-1):\n",
    "            # Your codes:\n",
    "            \n",
    "                    \n",
    "        # the last layer\n",
    "        layers.append(nn.Linear(hidden_sizes[num_hidden_layers-2], out_size))\n",
    "        \n",
    "        # sequentialization; otherwise the module can not access the parameters\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_by_accuracy(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():                    \n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.view(-1, 28*28).to(device), labels.to(device)\n",
    "            \n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)      \n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += int((predicted == labels).sum()) \n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_over_models(models_dict, writer, lr=0.01, epochs=30):\n",
    "    for model_name, model in models_dict.items():\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        for epoch in range(epochs):   \n",
    "\n",
    "            running_loss = 0.0\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.view(-1, 28*28).to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += float(loss)\n",
    "\n",
    "            # add the weights and the gradients of them in the first and last layers to TensorBoard's Histogram\n",
    "            for layer_no in (0, (model.num_hidden_layers-1)*2):\n",
    "                weight = model.layers[layer_no].weight\n",
    "                writer.add_histogram('weight/{}/{}'.format(model_name, layer_no//2+1), \n",
    "                                     weight.clone().cpu().data.numpy(), epoch)\n",
    "                writer.add_histogram('grad_weight/{}/{}'.format(model_name, layer_no//2+1),\n",
    "                                     weight.grad.clone().cpu().data.numpy(), epoch)\n",
    "            \n",
    "            # add evaluation on test set and loss on train set to TensorBoard's Scalar\n",
    "            acc = evaluate_by_accuracy(model)\n",
    "            writer.add_scalars('accuracy', {model_name: acc}, epoch+1)\n",
    "            writer.add_scalars('loss', {model_name: total_loss/(i+1)}, epoch+1)\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activation_functions(functions_dict, figsize=(12, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for name, func in functions_dict.items():\n",
    "        x = torch.arange(-5.0, 5.0, 1e-2, requires_grad=True)\n",
    "        y = func(x)\n",
    "        y.backward(torch.ones(x.size()))            # autograd\n",
    "        \n",
    "        plt.plot(x.data.numpy(), y.data.numpy(), '-', label=name, alpha=0.7)\n",
    "        plt.plot(x.data.numpy(), x.grad.data.numpy(), '--', label='derivative '+name, alpha=0.7)\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid vs. Hyperbolic Tangent (Tanh)\n",
    "\n",
    "\n",
    "||<center><img width=200/>Sigmoid</center>|<center><img width=200/>Tanh</center>|\n",
    "|-|-|-|\n",
    "|Expression|$$f(x) = \\frac{1}{1 + e^{-x}}$$|$$f(x)=\\frac{2}{1+e^{-2x}} - 1$$|\n",
    "|Range|(0, 1)|(-1, 1)|\n",
    "|Over the Origin|No|Yes|\n",
    "|Symmetry|Asymmetric|Centrosymmetric|\n",
    "|Derivative Function|$$f'(x) = f(x)\\left(1-f(x)\\right)$$ | $$ f'(x)=1-[f(x)]^2$$|\n",
    "|Range|(0, 0.25)|(0, 1]|\n",
    "\n",
    "Sigmoid function is especially used for predicting the probability.\n",
    "\n",
    "Tanh function is also sigmoidal (s-shaped), and mainly used in classification between two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_activation_functions({'sigmoid': torch.sigmoid,\n",
    "                           'tanh': torch.tanh})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell and go to your TensorBoard to observe the change of accuracies on test set and losses on train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_dict1 = {name: MLP(28*28, 10, activation=name).to(device) for name in ('sigmoid', 'tanh')}\n",
    "\n",
    "train_over_models(models_dict1, tensorboardX.SummaryWriter('./runs/05_shallow_mlp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid vs. Hyperbolic Tangent (Tanh) vs. Recified Linear Unit (ReLU) vs. Leaky ReLU\n",
    "\n",
    "\n",
    "||<center><img width=200/>Sigmoid</center>|<center><img width=200/>Tanh</center>|<center><img width=200/>ReLU</center>|<center><img width=200/>Leaky ReLU</center>\n",
    "|-|-|-|-|\n",
    "|Expression|$$f(x) = \\frac{1}{1 + e^{-x}}$$|$$f(x)=\\frac{2}{1+e^{-2x}} - 1$$|$$f(x)=\\max(0, x)$$|$$f(x)=\\begin{cases}\n",
    "\\displaystyle x & \\text{if  } x \\geq 0 \\\\ 0.01x & \\text{if  } x \\lt 0 \n",
    "\\end{cases}$$\n",
    "|Range|$(0, 1)$|$(-1, 1)$|$[0,\\infty]$|$[-\\infty, \\infty]$|\n",
    "|Symmetry|Asymmetric|Centrosymmetric|Asymmetric|Asymmetric|\n",
    "|Derivative Function|$$f'(x) = f(x)\\left(1-f(x)\\right)$$ | $$ f'(x)=1-[f(x)]^2$$| $$ f'(x) = \\begin{cases}\n",
    "\\displaystyle 1 & \\text{if  } x \\geq 0 \\\\ 0 & \\text{if  } x \\lt 0 \n",
    "\\end{cases}$$ | $$ f'(x) = \\begin{cases}\n",
    "\\displaystyle 1 & \\text{if  } x \\geq 0 \\\\ 0.01 & \\text{if  } x \\lt 0 \n",
    "\\end{cases}$$\n",
    "|Range|$(0, 0.25)$|$(0, 1]$|$\\{0, 1\\}$|$\\{0, 1\\}$\n",
    "|Continuous|Yes|Yes|No|No|\n",
    "|Vanishing Gradient|Yes|Yes|No?|Yes|\n",
    "|Saturation|Yes|Yes|No|No|\n",
    "|Dead Neurons|No|No|Yes|No|\n",
    "\n",
    "ReLU is used in almost all the convolutional neural networks or deep learning.\n",
    "\n",
    "Since ReLU force the negative values to be zero, the model may loss information from the data during training. Leaky ReLU can cover such a need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activation_functions({'relu': torch.relu})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activation_functions({'leaky_relu': F.leaky_relu})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell and go to your TensorBoard to observe the difference of the histograms of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict2 = {name: MLP(28*28, 10, hidden_size=48, num_hidden_layers=10, activation=name).to(device)\n",
    "                for name in ('sigmoid', 'tanh', 'relu', 'leaky_relu')}\n",
    "\n",
    "train_over_models(models_dict2, tensorboardX.SummaryWriter('./runs/05_deeper_mlp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper is better\n",
    "\n",
    "We can see that a deeper MLP model with ReLU will perform as well as a shallow one with Tanh, while a deeper one with Leaky ReLU will even work better than the others.\n",
    "\n",
    "### To compare the number of parameters between shallow models and deeper ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Shallow: ', count_parameters(models_dict1['sigmoid']))\n",
    "print('Deeper: ', count_parameters(models_dict2['sigmoid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information of other activation function candidates, please check [this site](https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLLecture",
   "language": "python",
   "name": "dllecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
